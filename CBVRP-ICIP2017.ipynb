{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Video Relevance Prediction Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Challenge is fully sponsored by [<img src=\"./logo/hulu_logo.png\" style=\"max-width:100%; width: 20%\"/>](https://www.hulu.com/) You can click the logo to visit Hulu.\n",
    "### [Register the Challenge](https://freeonlinesurveys.com/s/xl3le2ke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Video Relevance computation is one of the most important tasks for personalized online streaming service. Given the relevance of videos and the users watch/search behaviors, recommender system can provide personalized recommendations, which will help users to discover more contents of interests. In most online service, the computation of video relevance table is based on the users’ behavior. For example, given the watch behaviors of users, Matrix Factorization can be used to model the interaction between users and videos, then the features of videos from matrix factorization can be used to compute the video relevance tables. However, computing video relevance from the content directly, instead of as a byproduct from matrix factorization of user behaviors, is a more promising and challenging tasks, where the content of videos contains the pixels, audios, subtitles and metadata of videos. Since the content of videos contains almost all the information about a video, ideally, we can have enough information to build video relevance table only from contents. One advantage of content-based video relevance prediction is that the cold start problems can be alleviated, as we do not need the user behaviors for a new video. However, understanding the content of the videos is still a challenging task. The aim of this challenge is to explore efficient ways to do content-based video relevance prediction tasks for recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "The high level task of this challenge is to understand the content of videos for recommender system. For the purpose of a competition, we need to define specific tasks as well as the data used in the competition. \n",
    "### The configuration of the competition\n",
    "In this competition, the task is to learn a model that can compute the relevance between TV-Shows/Movies from the video contents and the metadata (e.g. actor/actress, director, descriptions, genres, etc.). Specifically, there are 2 sets of items: 1) **retrieval** set $\\mathcal{R}$; 2) **candidate** set $\\mathcal{C}$, where $|\\mathcal{R}|=R $ and $|\\mathcal{C}|=C $. For each item (TV Show/Movie) $r$ in the retrieval set, we will provide several trailers and the metadata associated with them. Click [here](https://youtu.be/KNiVxwXn9CU) to have a quick look of a trailer. Besides the trailers and metadata, we also provide a vector $v^{(r)}\\in \\mathbb{Z}^C$, for each item  in the retrieval set, where $v^{(r)}_c=m$ indicates that candidate item $c\\in \\mathcal{C}$ is the $m$<sup>th</sup> relevant item among all the $|C|$ candidate items in $\\mathcal{C}$, $m\\in\\lbrace1,2,\\ldots,M\\rbrace$ and $v^{(r)}_c=0$ indicates that candidate item $c$ is not among the top $M$ most relevant items of item $r$. We only provide the top $M=30$ relevant items for each item $r$. The relevance we provided is learned from massive user behaviors and can be treated as groundtruth. And there is no overlap between $\\mathcal{R}$ and $\\mathcal{C}$, i.e. $\\mathcal{R} \\cap \\mathcal{C}=\\Phi$. Due to the legal issue, we provide neither the videos nor the metadata of the items in the **candidate** set.\n",
    "\n",
    "We divide the **retrieval** set $\\mathcal{R}$ into training set, validation set and testing set. The participants are expected to learn a model to compute the relevance (similarity) $w_{r^{\\star},r}$ between item $r^{\\star}$ in the testing set and item $r$ in the training set, based on videos and metadata we provided, and then use the relevance $w_{r^{\\star},r}$ to compute a vector $s^{(r^{\\star})}\\in \\mathbb{R}^C$, where $s^{(r^{\\star})}_c$ is a score indicating the relevance between $r^{\\star}$ with the candidate item $c\\in \\mathcal{C}$. For example, one option could be that \n",
    "$$s^{(r^{\\star})}_c = \\frac{1}{N}\\sum_{r=1}^N w_{r^{\\star},r}\\frac{\\mathbb{1}_{v_c^{(r)}\\neq 0}}{v_c^{(r)} + \\epsilon}$$ where $N$ is the number of items in the training set, $\\mathbb{1}_{v_c^{(r)}\\neq 0}$ equals to $1$ if $v_c^{(r)}\\neq 0$, and $0$ otherwise. The participants are free and encouraged to find other way to compute $s^{(r^{\\star})}_c$.\n",
    "### The Metric\n",
    "To measure the performance of models, we use ranking loss $\\mathcal{L}$ [1,2] as the metric, which measures the difficulty to derive the desiring ranking by the prediction of the models. More specifically, suppose the groundtruth top $M$ most relevant items in candidate set for item $r$ in the testing set is $o^{(r)} = [o_1^{(r)},o_2^{(r)},...,o_M^{(r)}]$ with order, where $o_i^{(r)} \\in \\mathcal{C}$ is the item in the candidate set that ranked at $i$<sup>th</sup> position in $o^{(r)}$, and $s^{(r)}_c$ is score indicating the predicted relevance by the model between item $r$ and item $o_i^{(r)}$, then the ranking loss is defined as: $$\\mathcal{L}(s,o) = - \\sum_{i=1}^M \\log \\frac{\\exp(s_{o_i})}{\\sum_{j\\in \\mathcal{C}\\setminus o_{:i-1}}s_{j}}$$ where $\\mathcal{C}\\setminus o_{:i-1}$ is a set containing all the candidate items in $\\mathcal{C}$ except the first $i-1$ items in $o$. In the above equation, we ignore the superscript $(r)$ for simplicity. We provide the python [**script**](https://github.com/CBVRP-ICIP-2017/CBVRP-ICIP2017/blob/master/metric/cbvrp_icip_metric.py) to compute the ranking loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration and Dataset\n",
    "In this competition, the retrieval set contains several TV-Shows/Movies including several trailers and the corresponding metadata. Moreover, the retrieval set is divided into training, validation set, and testing set. For the items in the training and validation set, we also provide $v^{(r)}$ along with them.\n",
    "\n",
    "To get access to the dataset, the participants should fill in an application form. After submitting the application form, we will send the access link to the email address of the participant within 1 working day after **April 3<sup>rd</sup>**.  **Click [_here_](https://freeonlinesurveys.com/s/xl3le2ke) to register and get the access of the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "The participants should prepare a csv file containing $s^{(r)}$ for each item $r$ in the testing set, and send the csv file to cbvrp-icip2017@hulu.com. The format of csv file is $$r_1, s^{(r_1)}_1, s^{(r_1)}_2,...,s^{(r_1)}_C$$ $$r_2, s^{(r_2)}_1, s^{(r_2)}_2,...,s^{(r_2)}_C$$ $$...$$ $$r_L, s^{(r_L)}_1, s^{(r_L)}_2,...,s^{(r_L)}_C$$\n",
    "where $L$ is the number of testing samples, $r_i$ is the id of the testing sample, $C$ is the number of items in candidate set $\\mathcal{C}$. The order of the items in candidate set is the same as $v^{(r)}$. Clik [**here**](https://github.com/CBVRP-ICIP-2017/CBVRP-ICIP2017/blob/master/metric/prediction.csv) to have a look at a submission file example.\n",
    "\n",
    "After receiving the submission csv file, we will compute the ranking loss and send the results to the participants by email no later than May 31<sup>th</sup>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule\n",
    "* April 3<sup>rd</sup>: Dataset available to participants\n",
    "* May 17<sup>th</sup>: Deadline for submitting the results\n",
    "* May 17<sup>th</sup>: (Optional) Submission of paper describing the algorithm behind the submission results.  The paper should follow the format instruction of ICIP2017 http://2017.ieeeicip.org/PaperSubmission.asp. The submission will be under a peer review process of the program committee of ICIP2017. The accepted submission will be included in IEEE Xplore and distributed with USB proceedings at the conference. Hence **the participants gain the opportunity to have a publication on ICIP2017.** More details will come soon.\n",
    "* May 31<sup>st</sup>: Notification of the winners.\n",
    "* May 31<sup>st</sup>: Notification of acceptance of the peer reviewed paper submitted to CVBRP2017.\n",
    "* July 15<sup>st</sup>: Winners submit the tech report and source code.\n",
    "* September 17<sup>th</sup> ~ 20<sup>th</sup>: Presentation of the results at ICIP2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizers\n",
    "* [Yin Zheng](https://sites.google.com/site/zhengyin1126/) (yin.zheng@hulu.com) Hulu LLC.\n",
    "* Mengyi Liu (mengyi.liu@hulu.com) Hulu LLC.\n",
    "* Bangsheng Tang (bangsheng@hulu.com) Hulu LLC.\n",
    "* Xiaohui Xie (xiaohui.xie@hulu.com) Hulu LLC.\n",
    "* Hanning Zhou (eric.zhou@hulu.com) Hulu LLC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prizes\n",
    "The total reward is **$2,500** USD including the taxable amount, which will be fully sponsored by Hulu LLC. The number of winners will depend on the number of participants and the quality of the results. The organizers reserve the complete right in the final judgement and decision.\n",
    "\n",
    "The winners of the challenge are **required** to provide a technique report describing the details of the winning algorithms, and provide the source code to the organizers.\n",
    "The organizers will also run the released the code to test the reproducibility of the winner algorithms. The winners will give a presentation during the conference. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "If you have any question, please send email to cbvrp-icip2017@hulu.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "1.\tCan the participants make use other metadata?\n",
    "    * No. For fair comparison, the participants are required to use only the videos and metadata provided by organizers.\n",
    "        \n",
    "2. Since the challenge provide metadata and trailers, the participants are able to find out the name of the Movies/TV Shows. Can the participants use the relevance information of the Movies/TV Shows directly from other website, e.g. IMDB, Rotten Tomatoes, etc.\n",
    "    * No. The participants are not allowed to use the relevance information existed from other source.\n",
    "\n",
    "3. What is the form of the challenge, team work or individual participants?\n",
    "    * We do not limit on this point. The participants can be either team work or individuals. And the nubmer of team member is not limited.\n",
    "\n",
    "4. Can the participants use the dataset for other purpose other than the contest?\n",
    "    * No. The dataset we provided are only allowed to used by the participants for this challenge. The participants are not allowd to use it for other purpose and distribute it to others (persons/organizations).\n",
    "    \n",
    "5. Who will perserve the owenership of the algorithms, the participants or Hulu LLC.?\n",
    "    * The authors of the algorithms preserve the copyright of the algorithms. However, the winners should allowed Hulu LLC., the organizer of this challenge, to make use of the winning algorithms.\n",
    "\n",
    "6. I cannot open the hyper-link in this site.\n",
    "    * Due to some secure problems, github forbids to open the link directly by clicking. You can open the link in a new tab/window. Please contact us if you still cannot open the links in this page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "[1] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. \"Listwise approach to learning to rank: theory and algorithm.\" In Proceedings of the 25th International Conference on Machine Learning, pp. 1192-1199. 2008.\n",
    "\n",
    "[2] Yin Zheng, Bangsheng Tang, Wenkui Ding, and Hanning Zhou. \"A neural autoregressive approach to collaborative filtering.\" In Proceedings of the 33nd International Conference on Machine Learning, pp. 764-773. 2016.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
